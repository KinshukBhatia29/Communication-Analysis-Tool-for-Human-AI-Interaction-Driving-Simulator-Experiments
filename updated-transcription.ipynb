{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11226250,"sourceType":"datasetVersion","datasetId":7011662},{"sourceId":11309214,"sourceType":"datasetVersion","datasetId":7072949}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ffmpeg-python pandas nltk seaborn matplotlib openai-whisper pandas\n# ffmpeg-python: Used to extract audio from video files and convert them to mono 16kHz WAV format.\n# pandas: Used for data manipulation, creating dataframes, and exporting the final transcription and analysis results as a CSV.\n# nltk: Used for natural language processing tasks, specifically sentiment analysis using the VADER model.\n# seaborn: Used for visualizing data, such as sentiment distribution.\n# matplotlib: Used for plotting histograms and other visual representations of data.\n# openai-whisper: A pre-trained model used for automatic speech recognition (ASR) to generate transcriptions from audio.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T18:55:12.577108Z","iopub.execute_input":"2025-04-07T18:55:12.577413Z","iopub.status.idle":"2025-04-07T18:55:32.257677Z","shell.execute_reply.started":"2025-04-07T18:55:12.577366Z","shell.execute_reply":"2025-04-07T18:55:32.256779Z"}},"outputs":[{"name":"stdout","text":"Collecting ffmpeg-python\n  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\nCollecting openai-whisper\n  Downloading openai-whisper-20240930.tar.gz (800 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (1.0.0)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.67.1)\nRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.9.0)\nCollecting triton>=2.0.0 (from openai-whisper)\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2.4.1)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas) (2024.2.0)\nDownloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\nDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: openai-whisper\n  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803405 sha256=0e0a306cbf4707c8cad21406ab9dec0bf62ac987ab84eb208332603f0b28bc8d\n  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\nSuccessfully built openai-whisper\nInstalling collected packages: triton, ffmpeg-python, openai-whisper\nSuccessfully installed ffmpeg-python-0.2.0 openai-whisper-20240930 triton-3.2.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install \"pyannote.audio[all]\"\n!pip install torch torchvision torchaudio\n!pip install librosa\n# pyannote.audio[all]: Used for speaker diarization to identify and segment speakers within the audio locally.\n# torch: Core library required to run Whisper and pyannote.audio models.\n# torchvision: Included as part of the PyTorch ecosystem (not directly used here but required in some environments).\n# torchaudio: Handles audio processing within the PyTorch ecosystem.\n# librosa: Audio analysis library; helpful for processing and analyzing audio signals (optional but useful for extra features).","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T18:55:55.754941Z","iopub.execute_input":"2025-04-07T18:55:55.755252Z","iopub.status.idle":"2025-04-07T18:56:17.350848Z","shell.execute_reply.started":"2025-04-07T18:55:55.755224Z","shell.execute_reply":"2025-04-07T18:56:17.349770Z"}},"outputs":[{"name":"stdout","text":"Collecting pyannote.audio[all]\n  Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl.metadata (11 kB)\n\u001b[33mWARNING: pyannote-audio 3.3.2 does not provide the extra 'all'\u001b[0m\u001b[33m\n\u001b[0mCollecting asteroid-filterbanks>=0.4 (from pyannote.audio[all])\n  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (0.8.0)\nRequirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (0.29.0)\nCollecting lightning>=2.0.1 (from pyannote.audio[all])\n  Downloading lightning-2.5.1-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (2.3.0)\nCollecting pyannote.core>=5.0.0 (from pyannote.audio[all])\n  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\nCollecting pyannote.database>=5.0.1 (from pyannote.audio[all])\n  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\nCollecting pyannote.metrics>=3.2 (from pyannote.audio[all])\n  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\nCollecting pyannote.pipeline>=3.0.1 (from pyannote.audio[all])\n  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\nCollecting pytorch-metric-learning>=2.1.0 (from pyannote.audio[all])\n  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (13.9.4)\nRequirement already satisfied: semver>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (3.0.4)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (0.12.1)\nCollecting speechbrain>=1.0.0 (from pyannote.audio[all])\n  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\nCollecting tensorboardX>=2.6 (from pyannote.audio[all])\n  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (2.5.1+cu121)\nCollecting torch-audiomentations>=0.11.0 (from pyannote.audio[all])\n  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torchaudio>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (2.5.1+cu121)\nRequirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (1.6.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio[all]) (1.26.4)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio[all]) (4.12.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio[all]) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio[all]) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio[all]) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio[all]) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio[all]) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio[all]) (4.67.1)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio[all]) (0.12.0)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio[all]) (2.5.0.post0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio[all]) (4.9.3)\nRequirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio[all]) (2.4.0)\nRequirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio[all]) (1.13.1)\nRequirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio[all]) (2.2.3)\nRequirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio[all]) (0.15.1)\nRequirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio[all]) (1.2.2)\nCollecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio[all])\n  Downloading docopt-0.6.2.tar.gz (25 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio[all]) (0.9.0)\nRequirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio[all]) (3.7.5)\nRequirement already satisfied: sympy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio[all]) (1.13.1)\nRequirement already satisfied: optuna>=3.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.pipeline>=3.0.1->pyannote.audio[all]) (4.2.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio[all]) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio[all]) (2.19.1)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->pyannote.audio[all]) (1.17.1)\nCollecting hyperpyyaml (from speechbrain>=1.0.0->pyannote.audio[all])\n  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain>=1.0.0->pyannote.audio[all]) (1.4.2)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain>=1.0.0->pyannote.audio[all]) (0.2.0)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.6->pyannote.audio[all]) (3.20.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio[all]) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio[all]) (3.1.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.1->pyannote.metrics>=3.2->pyannote.audio[all]) (1.3.0)\nCollecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio[all])\n  Downloading julius-0.2.7.tar.gz (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio[all])\n  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio[all]) (2.22)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (3.11.12)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio[all]) (75.1.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio[all]) (0.1.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio[all]) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio[all]) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio[all]) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio[all]) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio[all]) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio[all]) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio[all]) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (2.4.1)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio[all]) (1.14.1)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio[all]) (6.9.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio[all]) (2.0.36)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio[all]) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio[all]) (2025.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio[all]) (3.5.0)\nCollecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio[all])\n  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio[all]) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio[all]) (1.5.4)\nCollecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio[all])\n  Downloading ruamel.yaml-0.18.10-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio[all]) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio[all]) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio[all]) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio[all]) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio[all]) (2025.1.31)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (1.18.3)\nRequirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio[all]) (1.3.9)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio[all]) (1.17.0)\nCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio[all])\n  Downloading ruamel.yaml.clib-0.2.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio[all]) (3.1.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (2024.2.0)\nDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\nDownloading lightning-2.5.1-py3-none-any.whl (818 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m818.9/818.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\nDownloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m120.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m32.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pyannote.audio-3.3.2-py2.py3-none-any.whl (898 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\nDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\nDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\nDownloading ruamel.yaml-0.18.10-py3-none-any.whl (117 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (722 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.2/722.2 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: docopt, julius\n  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=c4bb6650d93b877c97fd097f8f678f84cd845c743957951645b577e3834c2b7b\n  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21869 sha256=6e5cb21bb69ee031330cd1bd1fb0168933d7307c80bdb06c53f7630425917a7a\n  Stored in directory: /root/.cache/pip/wheels/b9/b2/05/f883527ffcb7f2ead5438a2c23439aa0c881eaa9a4c80256f4\nSuccessfully built docopt julius\nInstalling collected packages: primePy, docopt, ruamel.yaml.clib, ruamel.yaml, julius, hyperpyyaml, torch-pitch-shift, torch-audiomentations, pyannote.core, pyannote.database, tensorboardX, speechbrain, pytorch-metric-learning, pyannote.pipeline, pyannote.metrics, lightning, asteroid-filterbanks, pyannote.audio\nSuccessfully installed asteroid-filterbanks-0.4.0 docopt-0.6.2 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.5.1 primePy-1.3 pyannote.audio-3.3.2 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-metric-learning-2.8.1 ruamel.yaml-0.18.10 ruamel.yaml.clib-0.2.12 speechbrain-1.0.3 tensorboardX-2.6.2.2 torch-audiomentations-0.12.0 torch-pitch-shift-1.2.5\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\nRequirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\nRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\nRequirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\nRequirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2.4.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **Step-by-Step Workflow**","metadata":{}},{"cell_type":"markdown","source":"## **Step 1: Data Ingestion**\n#### Loads all video files, extracts audio, and converts it to mono 16kHz WAV format for further processing.\n\n## **Step 2: Transcription**\n#### Uses Whisper (offline model) to transcribe spoken words in the audio into text along with start and end timestamps.\n\n## **Step 3: Speaker Diarization**\n#### Uses a pre-trained local model (like pyannote) to assign speaker labels to different segments of the audio, distinguishing between different voices.\n\n## **Step 4: Time Bucketing**\n#### Segments each transcription line into 5-second intervals based on start time for better temporal analysis and aggregation.\n\n## **Step 5: Sentiment Analysis**\n#### Analyzes the sentiment of each transcribed text segment using NLTK's VADER model and labels them as positive, negative, or neutral.\n\n## **Step 6: Named Entity Recognition (NER)**\n#### Applies a Hugging Face NER model to detect and extract entities (like names, places, orgs) from each text segment.\n\n## **Step 7: Export to CSV**\n#### Combines all processed results (transcription, speakers, timestamps, sentiment, entities) into a single structured CSV for analysis or visualization.","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nimport pandas as pd\nimport nltk\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nimport ffmpeg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T18:57:51.250666Z","iopub.execute_input":"2025-04-07T18:57:51.250950Z","iopub.status.idle":"2025-04-07T18:57:51.292491Z","shell.execute_reply.started":"2025-04-07T18:57:51.250929Z","shell.execute_reply":"2025-04-07T18:57:51.291786Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"nltk.download('vader_lexicon')  # Required for sentiment analysis\nnltk.download('punkt')          # Tokenizer for text processing","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T18:58:51.328605Z","iopub.execute_input":"2025-04-07T18:58:51.328906Z","iopub.status.idle":"2025-04-07T18:58:51.471693Z","shell.execute_reply.started":"2025-04-07T18:58:51.328886Z","shell.execute_reply":"2025-04-07T18:58:51.470872Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# **Step 1: Data Ingestion (Video to Audio Conversion)** \n#### * Load video files from the input directory.\n#### * Extract audio from each video using ffmpeg.\n#### * Convert audio to mono and 16kHz .wav format.\n#### * Store the resulting audio files in an output folder (e.g., ./output_audio).","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\n\ndef extract_audio(video_path, output_audio_path):\n    \"\"\"\n    Extracts audio from a single video and saves it as mono 16kHz WAV.\n\n    Parameters:\n        video_path (str): Path to the input video file.\n        output_audio_path (str): Path where the output audio will be saved.\n\n    Returns:\n        str: Path to the saved audio file.\n    \"\"\"\n    cmd = [\n        \"ffmpeg\", \"-i\", video_path, \"-q:a\", \"0\", \"-map\", \"a\",\n        \"-ac\", \"1\", \"-ar\", \"16000\",  # Mono & 16kHz\n        output_audio_path, \"-y\"\n    ]\n    subprocess.run(cmd, capture_output=True, text=True)\n    return output_audio_path\n\ndef process_all_videos(input_video_dir):\n    \"\"\"\n    Processes all video files in the input directory,\n    extracts audio, and saves them to ./output_audio/.\n\n    Parameters:\n        input_video_dir (str): Directory containing video files.\n    \"\"\"\n    output_audio_dir = \"./output_audio\"\n    os.makedirs(output_audio_dir, exist_ok=True)\n\n    video_extensions = (\".mp4\", \".mov\", \".avi\", \".mkv\")\n\n    for filename in os.listdir(input_video_dir):\n        if filename.lower().endswith(video_extensions):\n            video_path = os.path.join(input_video_dir, filename)\n            base_name = os.path.splitext(filename)[0]\n            output_audio_path = os.path.join(output_audio_dir, f\"{base_name}.wav\")\n\n            extract_audio(video_path, output_audio_path)\n            print(f\"✅ Processed: {filename} → {output_audio_path}\")\n\n# 🟡 Call this function with your actual input folder containing videos\ninput_video_directory = \"Your input video directory\"\nprocess_all_videos(input_video_directory)  # Change path as needed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T18:59:34.768643Z","iopub.execute_input":"2025-04-07T18:59:34.768968Z","iopub.status.idle":"2025-04-07T18:59:39.323589Z","shell.execute_reply.started":"2025-04-07T18:59:34.768941Z","shell.execute_reply":"2025-04-07T18:59:39.322825Z"}},"outputs":[{"name":"stdout","text":"✅ Processed: Experimenter_CREW_999_1_All_1731617801.mp4 → ./output_audio/Experimenter_CREW_999_1_All_1731617801.wav\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# **Step 2: Transcription (Using Whisper)**\n#### * Load a pre-trained Whisper model (base, small, etc.).\n#### * Transcribe each .wav file from the audio output directory.\n#### * Store transcription segments with timestamps and text.","metadata":{}},{"cell_type":"code","source":"import os\nimport whisper\n\ndef transcribe_all_audio(audio_dir):\n    \"\"\"\n    Transcribes all .wav files in a directory using Whisper.\n\n    Parameters:\n        audio_dir (str): Path to the directory containing .wav files.\n\n    Returns:\n        dict: Mapping from audio filename to list of transcription segments.\n    \"\"\"\n    model = whisper.load_model(\"base\")  # You can try 'small' or 'medium' if GPU is available\n    transcripts = {}\n\n    for file in os.listdir(audio_dir):\n        if file.endswith(\".wav\"):\n            audio_path = os.path.join(audio_dir, file)\n            print(f\"🔍 Transcribing: {file} ...\")\n            result = model.transcribe(audio_path)\n            transcripts[file] = result[\"segments\"]  # Each segment: dict with 'start', 'end', 'text'\n            print(f\"✅ Transcribed: {file} → {len(result['segments'])} segments\")\n\n    return transcripts\n\n# 🟢 Run this on your extracted audio directory\naudio_dir = \"./output_audio\"\nall_transcriptions = transcribe_all_audio(audio_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T18:59:46.698322Z","iopub.execute_input":"2025-04-07T18:59:46.698651Z","iopub.status.idle":"2025-04-07T19:00:09.962236Z","shell.execute_reply.started":"2025-04-07T18:59:46.698611Z","shell.execute_reply":"2025-04-07T19:00:09.961423Z"}},"outputs":[{"name":"stderr","text":"100%|███████████████████████████████████████| 139M/139M [00:04<00:00, 32.7MiB/s]\n/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(fp, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"🔍 Transcribing: Experimenter_CREW_999_1_All_1731617801.wav ...\n✅ Transcribed: Experimenter_CREW_999_1_All_1731617801.wav → 78 segments\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### **Print the first 5 transcription segments from the first audio file**","metadata":{}},{"cell_type":"code","source":"first_audio_file = list(all_transcriptions.keys())[0]  # Get the first file name\nfirst_five_segments = all_transcriptions[first_audio_file][:5]  # First 5 segments\n\nfor i, segment in enumerate(first_five_segments, 1):\n    print(f\"Segment {i}:\")\n    print(f\"Start: {segment['start']}s\")\n    print(f\"End: {segment['end']}s\")\n    print(f\"Text: {segment['text']}\")\n    print(\"-\" * 40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T19:01:46.447889Z","iopub.execute_input":"2025-04-07T19:01:46.448368Z","iopub.status.idle":"2025-04-07T19:01:46.456520Z","shell.execute_reply.started":"2025-04-07T19:01:46.448342Z","shell.execute_reply":"2025-04-07T19:01:46.455567Z"}},"outputs":[{"name":"stdout","text":"Segment 1:\nStart: 0.0s\nEnd: 13.36s\nText:  Okay, so the drive you're going to complete and use the E-cautomation and the object detection\n----------------------------------------\nSegment 2:\nStart: 13.36s\nEnd: 17.400000000000002s\nText:  system so that it will not be to operate the vehicle, so keep your hands off this steering\n----------------------------------------\nSegment 3:\nStart: 17.400000000000002s\nEnd: 19.88s\nText:  wheel and meet off the pedals and punch me in that drive.\n----------------------------------------\nSegment 4:\nStart: 19.88s\nEnd: 20.88s\nText:  Okay.\n----------------------------------------\nSegment 5:\nStart: 20.88s\nEnd: 23.8s\nText:  So when you see that some driver in the caterer highlight green, make sure you don't get\n----------------------------------------\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# **Step 3: Speaker Diarization (Using Local Pyannote Pretrained Model)** \n#### * Use a pretrained diarization pipeline from pyannote.audio (e.g., pyannote/speaker-diarization).\n#### * Run speaker diarization on each .wav file.\n#### * Assign speaker labels (e.g., Speaker 0, Speaker 1) to time-stamped segments.\n#### * Merge speaker info with Whisper transcription based on time overlap.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=\"HUGGING FACE TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T19:03:10.152456Z","iopub.execute_input":"2025-04-07T19:03:10.152785Z","iopub.status.idle":"2025-04-07T19:03:10.903321Z","shell.execute_reply.started":"2025-04-07T19:03:10.152759Z","shell.execute_reply":"2025-04-07T19:03:10.902450Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from pyannote.audio import Pipeline\nfrom huggingface_hub import login\nfrom datetime import timedelta\n\n # Replace with your actual token\n\n# Load pretrained speaker diarization pipeline\npipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=True)\n\ndef diarize_audio_pyannote(audio_path):\n    \"\"\"\n    Perform speaker diarization using pyannote and return segments with speaker info.\n    \n    Parameters:\n        audio_path (str): Path to the WAV audio file.\n\n    Returns:\n        list of dicts: Speaker segments with start_time, end_time, speaker label.\n    \"\"\"\n    diarization = pipeline(audio_path, num_speakers=15)  # 💡 Force 15 speakers\n\n    segments = []\n    for turn, _, speaker in diarization.itertracks(yield_label=True):\n        segments.append({\n            \"start\": turn.start,\n            \"end\": turn.end,\n            \"speaker\": speaker\n        })\n    return segments\ndef assign_speakers_to_transcripts(whisper_segments, speaker_segments):\n    \"\"\"\n    Match Whisper transcription segments to speaker segments by overlap.\n\n    Parameters:\n        whisper_segments (list): List of Whisper segments with 'start', 'end', 'text'.\n        speaker_segments (list): List of diarization segments with 'start', 'end', 'speaker'.\n\n    Returns:\n        list of dicts: Each dict contains transcription + assigned speaker.\n    \"\"\"\n    enriched = []\n    for ws in whisper_segments:\n        speaker = \"Unknown\"\n        for ss in speaker_segments:\n            if ss[\"start\"] <= ws[\"start\"] < ss[\"end\"]:\n                speaker = ss[\"speaker\"]\n                break\n        enriched.append({\n            \"start_time\": round(ws[\"start\"], 2),\n            \"end_time\": round(ws[\"end\"], 2),\n            \"text\": ws[\"text\"],\n            \"speaker\": speaker\n        })\n    return enriched\nall_transcripts_with_speakers = {}\n\nfor file in os.listdir(audio_dir):\n    if file.endswith(\".wav\"):\n        audio_path = os.path.join(audio_dir, file)\n        base_name = os.path.splitext(file)[0]\n\n        print(f\"🎙️ Running diarization for {file}...\")\n        speaker_segs = diarize_audio_pyannote(audio_path)\n\n        whisper_segs = all_transcriptions[file]  # From Step 2\n        enriched = assign_speakers_to_transcripts(whisper_segs, speaker_segs)\n\n        all_transcripts_with_speakers[base_name] = enriched\n        print(\"-\" * 60)\n\n        for i, line in enumerate(enriched[:5], start=1):\n            print(f\"{i}. [{line['speaker']}] {line['text']} (Start: {line['start_time']}s, End: {line['end_time']}s)\")\n\n        print(\"-\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T19:03:27.087259Z","iopub.execute_input":"2025-04-07T19:03:27.087605Z","iopub.status.idle":"2025-04-07T19:27:33.108443Z","shell.execute_reply.started":"2025-04-07T19:03:27.087576Z","shell.execute_reply":"2025-04-07T19:27:33.107569Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.yaml:   0%|          | 0.00/500 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c64ea04da9c34cedab298072eab7fab4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/17.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89c7d401718f47ad824de34b42fcfb8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.yaml:   0%|          | 0.00/318 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a3cc3212c884b909718fe2460221f32"}},"metadata":{}},{"name":"stdout","text":"Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\nModel was trained with torch 1.10.0+cu102, yours is 2.5.1+cu121. Bad things might happen unless you revert torch to 1.x.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"hyperparams.yaml:   0%|          | 0.00/1.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc6b93be59b64281961dd6b5ad52fe90"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"embedding_model.ckpt:   0%|          | 0.00/83.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca2bfeac7fb5490b832fb5525457c2d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"mean_var_norm_emb.ckpt:   0%|          | 0.00/1.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0500905e103a4b0e877f0ccdb89d787a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"classifier.ckpt:   0%|          | 0.00/5.53M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f06f3e468ad4cb8af508dfa6e128d33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"label_encoder.txt:   0%|          | 0.00/129k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7423866d0ab543ed9b572cc10f034940"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\n/usr/local/lib/python3.10/dist-packages/speechbrain/processing/features.py:1529: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  stats = torch.load(path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"🎙️ Running diarization for Experimenter_CREW_999_1_All_1731617801.wav...\nFound only 5 clusters. Using a smaller value than 15 for `min_cluster_size` might help.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pyannote/audio/pipelines/speaker_diarization.py:554: UserWarning: \nThe detected number of speakers (5) is outside\nthe given bounds [15, 15]. This can happen if the\ngiven audio file is too short to contain 15 or more speakers.\nTry to lower the desired minimal number of speakers.\n\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Speaker assignment complete for Experimenter_CREW_999_1_All_1731617801.wav\n{'start_time': 0.0, 'end_time': 13.36, 'text': \" Okay, so the drive you're going to complete and use the E-cautomation and the object detection\", 'speaker': 'Unknown'}\n{'start_time': 13.36, 'end_time': 17.4, 'text': ' system so that it will not be to operate the vehicle, so keep your hands off this steering', 'speaker': 'SPEAKER_02'}\n{'start_time': 17.4, 'end_time': 19.88, 'text': ' wheel and meet off the pedals and punch me in that drive.', 'speaker': 'SPEAKER_02'}\n{'start_time': 19.88, 'end_time': 20.88, 'text': ' Okay.', 'speaker': 'SPEAKER_02'}\n{'start_time': 20.88, 'end_time': 23.8, 'text': \" So when you see that some driver in the caterer highlight green, make sure you don't get\", 'speaker': 'SPEAKER_02'}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(\"-\" * 60)\nfor i, line in enumerate(enriched[:5], start=1):\n    print(f\"{i}. [{line['speaker']}] {line['text']} (Start: {line['start_time']}s, End: {line['end_time']}s)\")\n\nprint(\"-\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T19:31:55.643226Z","iopub.execute_input":"2025-04-07T19:31:55.643574Z","iopub.status.idle":"2025-04-07T19:31:55.650346Z","shell.execute_reply.started":"2025-04-07T19:31:55.643547Z","shell.execute_reply":"2025-04-07T19:31:55.649458Z"}},"outputs":[{"name":"stdout","text":"------------------------------------------------------------\n1. [Unknown]  Okay, so the drive you're going to complete and use the E-cautomation and the object detection (Start: 0.0s, End: 13.36s)\n2. [SPEAKER_02]  system so that it will not be to operate the vehicle, so keep your hands off this steering (Start: 13.36s, End: 17.4s)\n3. [SPEAKER_02]  wheel and meet off the pedals and punch me in that drive. (Start: 17.4s, End: 19.88s)\n4. [SPEAKER_02]  Okay. (Start: 19.88s, End: 20.88s)\n5. [SPEAKER_02]  So when you see that some driver in the caterer highlight green, make sure you don't get (Start: 20.88s, End: 23.8s)\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# **Step 4: Bucket Transcriptions by Time**\n#### * Organize transcribed text into 5-second time buckets.\n#### * Each segment is assigned to a bucket_start and bucket_end (e.g., 0–5 sec, 5–10 sec).\n#### * Store all segments with their time buckets, speakers, and texts.","metadata":{}},{"cell_type":"code","source":"import math\n\ndef assign_buckets(transcript_segments, bucket_size=5):\n    \"\"\"\n    Adds 5-second time bucket information to each transcription segment.\n\n    Parameters:\n        transcript_segments (list): List of transcription segments with 'start_time' and 'end_time'.\n        bucket_size (int): Size of each time bucket in seconds (default: 5).\n\n    Returns:\n        list of dicts: Updated segments with bucket_start and bucket_end.\n    \"\"\"\n    for segment in transcript_segments:\n        start_bucket = int(math.floor(segment[\"start_time\"] / bucket_size)) * bucket_size\n        end_bucket = start_bucket + bucket_size\n        segment[\"bucket_start\"] = start_bucket\n        segment[\"bucket_end\"] = end_bucket\n    return transcript_segments\n# Dictionary to store the bucketed transcriptions\nbucketed_transcripts = {}\n# Assign buckets to all transcription segments\nfor file_name, segments in all_transcripts_with_speakers.items():\n    print(f\"Assigning buckets for: {file_name}\")\n    \n    # Assign buckets to the segments\n    updated_segments = assign_buckets(segments)\n    bucketed_transcripts[file_name] = updated_segments\n    \n    # Print the first 5 segments \n    print(f\"Buckets assigned. First 5 entries:\")\n    for i, segment in enumerate(updated_segments[:5], start=1):\n        print(f\"{i}. [Speaker: {segment['speaker']}] {segment['text']} (Start: {segment['start_time']}s, End: {segment['end_time']}s) → Bucket: {segment['bucket_start']}-{segment['bucket_end']}s\")\n\n    print(\"-\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T19:32:41.610352Z","iopub.execute_input":"2025-04-07T19:32:41.610709Z","iopub.status.idle":"2025-04-07T19:32:41.618221Z","shell.execute_reply.started":"2025-04-07T19:32:41.610684Z","shell.execute_reply":"2025-04-07T19:32:41.617604Z"}},"outputs":[{"name":"stdout","text":"Assigning buckets for: Experimenter_CREW_999_1_All_1731617801\nBuckets assigned. First 5 entries:\n1. [Speaker: Unknown]  Okay, so the drive you're going to complete and use the E-cautomation and the object detection (Start: 0.0s, End: 13.36s) → Bucket: 0-5s\n2. [Speaker: SPEAKER_02]  system so that it will not be to operate the vehicle, so keep your hands off this steering (Start: 13.36s, End: 17.4s) → Bucket: 10-15s\n3. [Speaker: SPEAKER_02]  wheel and meet off the pedals and punch me in that drive. (Start: 17.4s, End: 19.88s) → Bucket: 15-20s\n4. [Speaker: SPEAKER_02]  Okay. (Start: 19.88s, End: 20.88s) → Bucket: 15-20s\n5. [Speaker: SPEAKER_02]  So when you see that some driver in the caterer highlight green, make sure you don't get (Start: 20.88s, End: 23.8s) → Bucket: 20-25s\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# **Step 5: Sentiment Analysis (Using NLTK VADER)**\n#### * For each transcribed segment, analyze sentiment using the VADER sentiment analyzer.\n#### * Assign a label: positive, negative, or neutral based on compound score.\n#### * Add sentiment to the segment metadata.","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# Download NLTK VADER lexicon (only needed once)\nnltk.download(\"vader_lexicon\")\n\n# Initialize SentimentIntensityAnalyzer\nsia = SentimentIntensityAnalyzer()\n\ndef analyze_sentiment(text):\n    \"\"\"\n    Analyzes sentiment of a given text using VADER SentimentIntensityAnalyzer.\n\n    Parameters:\n        text (str): The text to analyze.\n\n    Returns:\n        str: Sentiment label: \"positive\", \"negative\", or \"neutral\".\n    \"\"\"\n    # Analyze sentiment\n    sentiment_score = sia.polarity_scores(text)[\"compound\"]\n\n    # Assign sentiment based on the compound score\n    if sentiment_score >= 0.05:\n        return \"positive\"\n    elif sentiment_score <= -0.05:\n        return \"negative\"\n    else:\n        return \"neutral\"\n\ndef apply_sentiment_to_segments(transcript_segments):\n    \"\"\"\n    Applies sentiment analysis to each transcription segment.\n\n    Parameters:\n        transcript_segments (list): List of transcription segments with 'text' and other info.\n\n    Returns:\n        list: Updated transcription segments with sentiment.\n    \"\"\"\n    for segment in transcript_segments:\n        segment[\"sentiment\"] = analyze_sentiment(segment[\"text\"])\n    return transcript_segments\n\n# Dictionary to store sentiment-analyzed transcripts\nsentimented_transcripts = {}\n\n# Apply sentiment analysis for all files\nfor file_name, segments in bucketed_transcripts.items():\n    print(f\" Analyzing sentiment for: {file_name}\")\n    \n    # Apply sentiment analysis to segments\n    updated_segments_with_sentiment = apply_sentiment_to_segments(segments)\n    sentimented_transcripts[file_name] = updated_segments_with_sentiment\n    \n    # Print the first 5 segments\n    print(f\"Sentiment analysis complete. First 5 segments:\")\n    \n    for i, entry in enumerate(updated_segments_with_sentiment[:5], start=1):\n        print(f\"{i}. [Speaker: {entry['speaker']}] {entry['text']} (Sentiment: {entry['sentiment']})\")\n        print(f\"   Start Time: {entry['start_time']}s, End Time: {entry['end_time']}s\")\n        print(f\"   Bucket: {entry['bucket_start']} - {entry['bucket_end']}s\")\n    print(\"-\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T19:32:46.205586Z","iopub.execute_input":"2025-04-07T19:32:46.205879Z","iopub.status.idle":"2025-04-07T19:32:46.226954Z","shell.execute_reply.started":"2025-04-07T19:32:46.205854Z","shell.execute_reply":"2025-04-07T19:32:46.226207Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n Analyzing sentiment for: Experimenter_CREW_999_1_All_1731617801\nSentiment analysis complete. First 5 segments:\n1. [Speaker: Unknown]  Okay, so the drive you're going to complete and use the E-cautomation and the object detection (Sentiment: positive)\n   Start Time: 0.0s, End Time: 13.36s\n   Bucket: 0 - 5s\n2. [Speaker: SPEAKER_02]  system so that it will not be to operate the vehicle, so keep your hands off this steering (Sentiment: neutral)\n   Start Time: 13.36s, End Time: 17.4s\n   Bucket: 10 - 15s\n3. [Speaker: SPEAKER_02]  wheel and meet off the pedals and punch me in that drive. (Sentiment: neutral)\n   Start Time: 17.4s, End Time: 19.88s\n   Bucket: 15 - 20s\n4. [Speaker: SPEAKER_02]  Okay. (Sentiment: positive)\n   Start Time: 19.88s, End Time: 20.88s\n   Bucket: 15 - 20s\n5. [Speaker: SPEAKER_02]  So when you see that some driver in the caterer highlight green, make sure you don't get (Sentiment: positive)\n   Start Time: 20.88s, End Time: 23.8s\n   Bucket: 20 - 25s\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# **Step 6: Named Entity Recognition (NER)**\n#### * Use a pre-trained Hugging Face model (e.g., dslim/bert-base-NER).\n#### * Detect named entities (people, organizations, places) in each transcription segment.\n#### * Store named entities alongside their corresponding segment.","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nfrom transformers import pipeline\n\n# Initialize the Hugging Face NER pipeline\nner_model = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n\n# Function to extract named entities from text\ndef extract_named_entities(text):\n    \"\"\"\n    Extracts named entities from the given text using a pre-trained NER model.\n\n    Parameters:\n        text (str): The input text to analyze.\n\n    Returns:\n        list: A list of named entities found in the text.\n    \"\"\"\n    ner_results = ner_model(text)\n    # Extract words (entities) from the NER output\n    named_entities = [result['word'] for result in ner_results]\n    return named_entities\n\n# Function to apply NER to each transcription segment\ndef apply_ner_to_segments(transcript_segments):\n    \"\"\"\n    Applies Named Entity Recognition (NER) to each transcription segment.\n\n    Parameters:\n        transcript_segments (list): List of transcription segments.\n\n    Returns:\n        list: Updated transcription segments with named entities.\n    \"\"\"\n    for segment in transcript_segments:\n        # Apply NER to each segment's text\n        segment[\"named_entities\"] = extract_named_entities(segment[\"text\"])\n    return transcript_segments\n\n# Example usage\n# Assuming `sentimented_transcripts` is the dictionary containing all transcribed segments along with sentiment\nnered_transcripts = {}\n\nfor file_name, segments in sentimented_transcripts.items():\n    print(f\"🔍 Extracting named entities for: {file_name}\")\n    updated_segments_with_ner = apply_ner_to_segments(segments)\n    nered_transcripts[file_name] = updated_segments_with_ner\n    print(f\"✅ NER complete for {file_name}. First 3 segments with named entities:\")\n    \n    # Display the first 3 segments with named entities\n    for entry in updated_segments_with_ner[:3]:\n        print(entry)\nfor file_name, segments in nered_transcripts.items():\n    print(f\"\\n📄 First 5 segments with Named Entities from {file_name}:\")\n    for i, entry in enumerate(segments[:5], start=1):\n        print(f\"{i}. [Speaker: {entry['speaker']}] {entry['text']} (Sentiment: {entry['sentiment']})\")\n        print(f\"   Named Entities: {', '.join(entry['named_entities'])}\")\n        print(f\"   Start Time: {entry['start_time']}s, End Time: {entry['end_time']}s\")\n        print(f\"   Bucket: {entry['bucket_start']} - {entry['bucket_end']}s\")\n    print(\"-\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T19:32:58.890056Z","iopub.execute_input":"2025-04-07T19:32:58.890343Z","iopub.status.idle":"2025-04-07T19:33:27.765887Z","shell.execute_reply.started":"2025-04-07T19:32:58.890319Z","shell.execute_reply":"2025-04-07T19:33:27.764990Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dac3a4fe24254bbb9310ded3cf644b00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f23182691a7046e29208fa504d24c04d"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71d5fa7cd34f4ac6931c72e60cde3f49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f5cdf149a9b43c9a13972f43ffc1689"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"🔍 Extracting named entities for: Experimenter_CREW_999_1_All_1731617801\n","output_type":"stream"},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"✅ NER complete for Experimenter_CREW_999_1_All_1731617801. First 3 segments with named entities:\n{'start_time': 0.0, 'end_time': 13.36, 'text': \" Okay, so the drive you're going to complete and use the E-cautomation and the object detection\", 'speaker': 'Unknown', 'bucket_start': 0, 'bucket_end': 5, 'sentiment': 'positive', 'named_entities': []}\n{'start_time': 13.36, 'end_time': 17.4, 'text': ' system so that it will not be to operate the vehicle, so keep your hands off this steering', 'speaker': 'SPEAKER_02', 'bucket_start': 10, 'bucket_end': 15, 'sentiment': 'neutral', 'named_entities': []}\n{'start_time': 17.4, 'end_time': 19.88, 'text': ' wheel and meet off the pedals and punch me in that drive.', 'speaker': 'SPEAKER_02', 'bucket_start': 15, 'bucket_end': 20, 'sentiment': 'neutral', 'named_entities': []}\n\n📄 First 5 segments with Named Entities from Experimenter_CREW_999_1_All_1731617801:\n1. [Speaker: Unknown]  Okay, so the drive you're going to complete and use the E-cautomation and the object detection (Sentiment: positive)\n   Named Entities: \n   Start Time: 0.0s, End Time: 13.36s\n   Bucket: 0 - 5s\n2. [Speaker: SPEAKER_02]  system so that it will not be to operate the vehicle, so keep your hands off this steering (Sentiment: neutral)\n   Named Entities: \n   Start Time: 13.36s, End Time: 17.4s\n   Bucket: 10 - 15s\n3. [Speaker: SPEAKER_02]  wheel and meet off the pedals and punch me in that drive. (Sentiment: neutral)\n   Named Entities: \n   Start Time: 17.4s, End Time: 19.88s\n   Bucket: 15 - 20s\n4. [Speaker: SPEAKER_02]  Okay. (Sentiment: positive)\n   Named Entities: \n   Start Time: 19.88s, End Time: 20.88s\n   Bucket: 15 - 20s\n5. [Speaker: SPEAKER_02]  So when you see that some driver in the caterer highlight green, make sure you don't get (Sentiment: positive)\n   Named Entities: \n   Start Time: 20.88s, End Time: 23.8s\n   Bucket: 20 - 25s\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# **Step 7: Export Results to CSV**\n#### * Combine all enriched segment data into rows (start time, end time, speaker, text, sentiment, named entities, etc.).\n#### * Export the final dataset into a structured .csv file.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef export_to_csv(nered_transcripts, output_file):\n    \"\"\"\n    Exports the transcription data with NER and sentiment information into a CSV file.\n\n    Parameters:\n        nered_transcripts (dict): Dictionary of transcriptions with sentiment and NER data.\n        output_file (str): Path to the output CSV file where data will be saved.\n    \"\"\"\n    rows = []\n\n    # Iterate through all transcriptions and extract relevant data\n    for file_name, segments in nered_transcripts.items():\n        for segment in segments:\n            row = {\n                \"start_time\": segment[\"start_time\"],\n                \"end_time\": segment[\"end_time\"],\n                \"bucket_start\": int(segment[\"start_time\"] // 5) * 5,  # 5-second bucket start\n                \"bucket_end\": (int(segment[\"start_time\"] // 5) + 1) * 5,  # 5-second bucket end\n                \"text\": segment[\"text\"],\n                \"sentiment\": segment[\"sentiment\"],\n                \"named_entities\": \", \".join(segment[\"named_entities\"]),  # Join named entities in a string\n                \"word_count\": len(segment[\"text\"].split()),  # Count words in the segment\n                \"speaker\": segment[\"speaker\"]\n            }\n            rows.append(row)\n\n    # Convert the rows to a DataFrame\n    df = pd.DataFrame(rows)\n\n    # Export the DataFrame to a CSV file\n    df.to_csv(output_file, index=False)\n    print(f\"✅ Data has been successfully exported to {output_file}\")\n\n# Example usage\noutput_csv_path = \"transcriptions_with_ner_and_sentiment.csv\"\nexport_to_csv(nered_transcripts, output_csv_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T19:34:17.609309Z","iopub.execute_input":"2025-04-07T19:34:17.609636Z","iopub.status.idle":"2025-04-07T19:34:17.619159Z","shell.execute_reply.started":"2025-04-07T19:34:17.609610Z","shell.execute_reply":"2025-04-07T19:34:17.618316Z"}},"outputs":[{"name":"stdout","text":"✅ Data has been successfully exported to transcriptions_with_ner_and_sentiment.csv\n","output_type":"stream"}],"execution_count":20}]}