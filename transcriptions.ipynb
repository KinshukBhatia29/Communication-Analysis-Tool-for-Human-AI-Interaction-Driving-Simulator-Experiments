{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11226250,"sourceType":"datasetVersion","datasetId":7011662}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk\nnltk.download('vader_lexicon')  # Required for sentiment analysis\nnltk.download('punkt')          # Tokenizer for text processing","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T16:00:55.969647Z","iopub.execute_input":"2025-04-08T16:00:55.969901Z","iopub.status.idle":"2025-04-08T16:00:55.977494Z","shell.execute_reply.started":"2025-04-08T16:00:55.969879Z","shell.execute_reply":"2025-04-08T16:00:55.976817Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"\ndef process_videos_pipeline(\n    input_video_dir=\"input video directory path\",\n    huggingface_token=\"Your hugging face api\",\n    output_audio_dir=\"output audio directiry path if any\",\n    output_csv_dir=\"output transcripted result directory path\",\n    whisper_model_size=\"base\",\n    bucket_size=5,#secofbucket\n    max_speakers=15 #maxspeakers\n):\n    import os\n    import subprocess\n    import math\n    import pandas as pd\n    import nltk\n    from nltk.sentiment import SentimentIntensityAnalyzer\n    import whisper\n    from transformers import pipeline\n    from pyannote.audio import Pipeline as DiarizationPipeline\n    from huggingface_hub import login\n\n    nltk.download('vader_lexicon')\n    nltk.download('punkt')\n\n    # --- Helper Functions ---\n    def extract_audio(video_path, output_audio_path):\n        cmd = [\n            \"ffmpeg\", \"-i\", video_path, \"-q:a\", \"0\", \"-map\", \"a\",\n            \"-ac\", \"1\", \"-ar\", \"16000\", output_audio_path, \"-y\"\n        ]\n        subprocess.run(cmd, capture_output=True, text=True)\n        return output_audio_path\n\n    def process_all_videos():\n        os.makedirs(output_audio_dir, exist_ok=True)\n        for filename in os.listdir(input_video_dir):\n            if filename.lower().endswith((\".mp4\", \".mov\", \".avi\", \".mkv\")):\n                video_path = os.path.join(input_video_dir, filename)\n                output_path = os.path.join(output_audio_dir, f\"{os.path.splitext(filename)[0]}.wav\")\n                extract_audio(video_path, output_path)\n                print(f\"‚úÖ Extracted: {filename} ‚Üí {output_path}\")\n\n    def transcribe_all_audio():\n        model = whisper.load_model(whisper_model_size)\n        transcripts = {}\n        for file in os.listdir(output_audio_dir):\n            if file.endswith(\".wav\"):\n                path = os.path.join(output_audio_dir, file)\n                print(f\"üîç Transcribing {file}...\")\n                result = model.transcribe(path)\n                transcripts[file] = result[\"segments\"]\n        return transcripts\n\n    def diarize_audio(audio_path, diar_pipeline):\n        diarization = diar_pipeline(audio_path, num_speakers=max_speakers)\n        return [\n            {\"start\": turn.start, \"end\": turn.end, \"speaker\": speaker}\n            for turn, _, speaker in diarization.itertracks(yield_label=True)\n        ]\n\n    def assign_speakers_to_transcripts(whisper_segs, speaker_segs):\n        enriched = []\n        for ws in whisper_segs:\n            speaker = \"Unknown\"\n            for ss in speaker_segs:\n                if ss[\"start\"] <= ws[\"start\"] < ss[\"end\"]:\n                    speaker = ss[\"speaker\"]\n                    break\n            enriched.append({\n                \"start_time\": round(ws[\"start\"], 2),\n                \"end_time\": round(ws[\"end\"], 2),\n                \"text\": ws[\"text\"],\n                \"speaker\": speaker\n            })\n        return enriched\n\n    def assign_buckets(segments):\n        for seg in segments:\n            start = int(math.floor(seg[\"start_time\"] / bucket_size)) * bucket_size\n            seg[\"bucket_start\"] = start\n            seg[\"bucket_end\"] = start + bucket_size\n        return segments\n\n    def analyze_sentiment(text):\n        score = sia.polarity_scores(text)[\"compound\"]\n        return \"positive\" if score >= 0.05 else \"negative\" if score <= -0.05 else \"neutral\"\n\n    def apply_sentiment(segments):\n        for seg in segments:\n            seg[\"sentiment\"] = analyze_sentiment(seg[\"text\"])\n        return segments\n\n    def extract_named_entities(text):\n        return [entity[\"word\"] for entity in ner_pipeline(text)]\n\n    def apply_ner(segments):\n        for seg in segments:\n            seg[\"named_entities\"] = extract_named_entities(seg[\"text\"])\n        return segments\n\n    def export_per_video_csvs(nered_transcripts, output_dir=output_csv_dir):\n        os.makedirs(output_dir, exist_ok=True)\n        for file_name, segments in nered_transcripts.items():\n            rows = []\n            for segment in segments:\n                row = {\n                    \"start_time\": segment[\"start_time\"],\n                    \"end_time\": segment[\"end_time\"],\n                    \"bucket_start\": int(segment[\"start_time\"] // bucket_size) * bucket_size,\n                    \"bucket_end\": (int(segment[\"start_time\"] // bucket_size) + 1) * bucket_size,\n                    \"text\": segment[\"text\"],\n                    \"sentiment\": segment[\"sentiment\"],\n                    \"named_entities\": \", \".join(segment[\"named_entities\"]),\n                    \"word_count\": len(segment[\"text\"].split()),\n                    \"speaker\": segment[\"speaker\"]\n                }\n                rows.append(row)\n\n            df = pd.DataFrame(rows)\n            output_path = os.path.join(output_dir, f\"{file_name}.csv\")\n            df.to_csv(output_path, index=False)\n            print(f\"‚úÖ Exported: {output_path}\")\n\n    # --- Start Pipeline ---\n    process_all_videos()\n    transcriptions = transcribe_all_audio()\n\n    login(token=huggingface_token)\n    diar_pipeline = DiarizationPipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=True)\n\n    sia = SentimentIntensityAnalyzer()\n    ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n\n    all_data = {}\n\n    for fname, whisper_segs in transcriptions.items():\n        print(f\"üéô Processing file: {fname}\")\n        audio_path = os.path.join(output_audio_dir, fname)\n        speaker_segs = diarize_audio(audio_path, diar_pipeline)\n        enriched = assign_speakers_to_transcripts(whisper_segs, speaker_segs)\n        enriched = assign_buckets(enriched)\n        enriched = apply_sentiment(enriched)\n        enriched = apply_ner(enriched)\n        all_data[os.path.splitext(fname)[0]] = enriched\n        \n    export_per_video_csvs(all_data, output_csv_dir)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:18:23.430680Z","iopub.execute_input":"2025-04-09T12:18:23.430996Z","iopub.status.idle":"2025-04-09T12:18:23.448408Z","shell.execute_reply.started":"2025-04-09T12:18:23.430970Z","shell.execute_reply":"2025-04-09T12:18:23.447547Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"process_videos_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:18:31.416194Z","iopub.execute_input":"2025-04-09T12:18:31.416530Z","iopub.status.idle":"2025-04-09T12:42:43.241883Z","shell.execute_reply.started":"2025-04-09T12:18:31.416496Z","shell.execute_reply":"2025-04-09T12:42:43.241100Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n  warnings.warn(\"The twython library has not been installed. \"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n‚úÖ Extracted: Experimenter_CREW_999_1_All_1731617801.mp4 ‚Üí ./output_audio/Experimenter_CREW_999_1_All_1731617801.wav\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139M/139M [00:00<00:00, 180MiB/s]\n/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(fp, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"üîç Transcribing Experimenter_CREW_999_1_All_1731617801.wav...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.yaml:   0%|          | 0.00/500 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aefc42baa07b4051a628f9205a825029"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/17.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"449c961e2a5d47dcba01d1d37938559c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.yaml:   0%|          | 0.00/318 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"892486061f4c4452bef2030812c83e0c"}},"metadata":{}},{"name":"stdout","text":"Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\nModel was trained with torch 1.10.0+cu102, yours is 2.5.1+cu121. Bad things might happen unless you revert torch to 1.x.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"hyperparams.yaml:   0%|          | 0.00/1.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"061b7dd0bad449a0896773598afd16f3"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"embedding_model.ckpt:   0%|          | 0.00/83.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"299dfde0bde9419fab93dab448c7b6df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"mean_var_norm_emb.ckpt:   0%|          | 0.00/1.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4dd49831b5c4dc38668fc551690bd17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"classifier.ckpt:   0%|          | 0.00/5.53M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bbec510bc4c42e1bb1914258defefe7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"label_encoder.txt:   0%|          | 0.00/129k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"593dab53dd3644e4afdce46772aba1f5"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\n/usr/local/lib/python3.10/dist-packages/speechbrain/processing/features.py:1529: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  stats = torch.load(path, map_location=device)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f6b4f50ef024535b9b3d48d14c5b4db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78d84e2e401c4c9b9bc842ce8febd0f5"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96baffa246404577ab968b4c605dfcdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfb3987f92754b018b762454195bc7ab"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"üéô Processing file: Experimenter_CREW_999_1_All_1731617801.wav\nFound only 5 clusters. Using a smaller value than 15 for `min_cluster_size` might help.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pyannote/audio/pipelines/speaker_diarization.py:554: UserWarning: \nThe detected number of speakers (5) is outside\nthe given bounds [15, 15]. This can happen if the\ngiven audio file is too short to contain 15 or more speakers.\nTry to lower the desired minimal number of speakers.\n\n  warnings.warn(\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Exported: ./csv_outputs/Experimenter_CREW_999_1_All_1731617801.csv\n","output_type":"stream"}],"execution_count":4}]}